@misc{whisper,
      title={Robust Speech Recognition via Large-Scale Weak Supervision},
      author={Alec Radford and Jong Wook Kim and Tao Xu and Greg Brockman and Christine McLeavey and Ilya Sutskever},
      date={2022},
      eprint={2212.04356},
      archivePrefix={arXiv},
      primaryClass={eess.AS},
      url={https://arxiv.org/abs/2212.04356},
}

@inproceedings{gao2023funasr,
  author={Zhifu Gao and Zerui Li and Jiaming Wang and Haoneng Luo and Xian Shi and Mengzhe Chen and Yabin Li and Lingyun Zuo and Zhihao Du and Zhangyu Xiao and Shiliang Zhang},
  title={FunASR: A Fundamental End-to-End Speech Recognition Toolkit},
  date={2023},
  booktitle={INTERSPEECH},
}

@misc{foodsky,
      title={FoodSky: A Food-oriented Large Language Model that Passes the Chef and Dietetic Examination}, 
      author={Pengfei Zhou and Weiqing Min and Chaoran Fu and Ying Jin and Mingyu Huang and Xiangyang Li and Shuhuan Mei and Shuqiang Jiang},
      date={2024},
      eprint={2406.10261},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.10261}, 
}

@misc{qwen3,
  title={Qwen3 Technical Report}, 
  author={Qwen Team},
  date= 2025,
  eprint={2505.09388},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2505.09388}, 
}

@inproceedings{hanlp,
    title = "The Stem Cell Hypothesis: Dilemma behind Multi-Task Learning with Transformer Encoders",
    author = "He, Han and Choi, Jinho D.",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    date = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.451",
    pages = "5555--5577",
    abstract = "Multi-task learning with transformer encoders (MTL) has emerged as a powerful technique to improve performance on closely-related tasks for both accuracy and efficiency while a question still remains whether or not it would perform as well on tasks that are distinct in nature. We first present MTL results on five NLP tasks, POS, NER, DEP, CON, and SRL, and depict its deficiency over single-task learning. We then conduct an extensive pruning analysis to show that a certain set of attention heads get claimed by most tasks during MTL, who interfere with one another to fine-tune those heads for their own objectives. Based on this finding, we propose the Stem Cell Hypothesis to reveal the existence of attention heads naturally talented for many tasks that cannot be jointly trained to create adequate embeddings for all of those tasks. Finally, we design novel parameter-free probes to justify our hypothesis and demonstrate how attention heads are transformed across the five tasks during MTL through label analysis.",
}

@misc{XiaChuFang,
      title={Counterfactual Recipe Generation: Exploring Compositional Generalization in a Realistic Scenario}, 
      author={Xiao Liu and Yansong Feng and Jizhi Tang and Chengang Hu and Dongyan Zhao},
      date="{2022}",
      eprint={2210.11431},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2210.11431}, 
}

@article{ffmpeg,
  title={Converting video formats with FFmpeg},
  author={Tomar, Suramya},
  journal={Linux Journal},
  volume={2006},
  number={146},
  pages={10},
  date={2006},
  publisher={Belltown Media}
}
